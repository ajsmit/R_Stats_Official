\documentclass[10pt,a4,]{article}
\usepackage[dvipsnames]{xcolor}
% \RequirePackage[l2tabu, orthodox]{nag}
\usepackage[a4paper,text={16.5cm,25.2cm},centering,margin=2.4cm]{geometry}
% \usepackage[left=1.0in,top=1.0in,right=1.0in,bottom=1.0in]{geometry}
\newcommand*{\authorfont}{\fontfamily{phv}\selectfont}
\usepackage{hyperref,amsmath,amssymb,bm,url,enumitem,dcolumn,upquote,framed,alltt,textgreek,xfrac,fixltx2e}
\usepackage[australian]{babel}
\usepackage[compact,small]{titlesec}
\setlength{\parskip}{1.2ex}
\setlength{\parindent}{0em}

\def\tightlist{}

\usepackage{ifxetex}
\ifxetex
 \usepackage{fontspec}
 \defaultfontfeatures{Ligatures=TeX} % To support LaTeX quoting style
 % \defaultfontfeatures{Ligatures=TeX,Numbers={OldStyle}}
 \setmainfont{Palatino}
 \setsansfont[Scale=MatchLowercase]{Avenir Next Condensed}
 \setmonofont[Scale=0.78,Color=RubineRed]{Bitstream Vera Sans Mono}
\else
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{lmodern}
  % \usepackage[full]{textcomp} % directly use the degree (and some other) symbol
\fi

% place after fonts; even better typesetting for improved readability:
\graphicspath{ {figure/} }
\usepackage{tabularx} % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\usepackage{siunitx}
    \sisetup{%
        detect-mode,
        group-digits            = false,
        input-symbols           = ( ) [ ] - + < > *,
        table-align-text-post   = false,
        round-mode              = places,
        round-precision         = 3
        }
% \usepackage[font={small, sf}, labelfont=bf]{caption} % tweaking the captions
\usepackage[font={small}, labelfont=bf]{caption} % tweaking the captions
\usepackage[color=yellow, textsize=tiny]{todonotes}
\frenchspacing%

\usepackage{abstract}
\renewcommand{\abstractname}{} % clear the title
\renewcommand{\absnamepos}{empty} % originally center

\renewenvironment{abstract}
{{%
\setlength{\leftmargin}{0mm}
\setlength{\rightmargin}{\leftmargin}%
}%
\relax}
{\endlist}

\makeatletter
\def\@maketitle{%
\newpage
%  \null
%  \vskip 2em%
%  \begin{center}%
\let \footnote \thanks
 {\fontsize{18}{20}\selectfont\raggedright  \setlength{\parindent}{0pt} \@title \par}%
}
%\fi
\makeatother


\setcounter{secnumdepth}{0}





\usepackage{longtable,booktabs}
\setlength\heavyrulewidth{0.1em}
\setlength\lightrulewidth{0.0625em}


\title{What to do when assumptions about your data fail  }

\author{\Large AJ
Smit\vspace{0.05in} \newline\normalsize\emph{University of the Western
Cape}  }

\date{}

% PENALTIES
\widowpenalty=1000
\clubpenalty=1000
\doublehyphendemerits=9999 % Almost no consecutive line hyphens
\brokenpenalty=10000 % No broken words across columns/pages
\interfootnotelinepenalty=9999 % Almost never break footnotes

% SECTION, SUBSECETC.TITLES
\usepackage[compact]{titlesec}
\titleformat{\chapter}
  {\normalfont\LARGE\sffamily\bfseries}
  {\thechapter}
  {1em}
  {}
\titleformat{\section}
  {\normalfont\LARGE\sffamily\bfseries}
  {\thesection}
  {1em}
  {}
\titleformat{\subsection}
  {\normalfont\Large\sffamily\bfseries}
  {\thesubsection}
  {1em}
  {}
\titleformat{\subsubsection}
  {\normalfont\large\sffamily\bfseries\slshape}
  {\thesubsubsection}
  {1em}
  {}
% \titlespacing*{<command>}{<left>}{<before-sep>}{<after-sep>}
\titlespacing*{\chapter}
  {0pt}
  {1.2ex plus 1ex minus .2ex}
  {0.5ex plus .1ex minus .1ex}
\titlespacing*{\section}
  {0pt}
  {1.2ex plus 1ex minus .2ex}
  {0.5ex plus .1ex minus .1ex}
\titlespacing*{\subsection}
  {0pt}
  {1.2ex plus 1ex minus .2ex}
  {0.5ex plus .1ex minus .1ex}
\titlespacing*{\subsubsection}
  {0pt}
  {1.2ex plus 1ex minus .2ex}
  {0.5ex plus .1ex minus .1ex}




\newtheorem{hypothesis}{Hypothesis}
\usepackage{setspace}

\makeatletter

\@ifpackageloaded{hyperref}{}{%
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[colorlinks=true, citecolor=blue, linkcolor=cyan, pdfborder={0 0 0 }, unicode=true]{hyperref} % place after other packages, but before cleveref
\fi
}

\@ifpackageloaded{color}{
    \PassOptionsToPackage{usenames,dvipsnames}{color}
}{%
    \usepackage[usenames,dvipsnames]{color}
}

\usepackage{cleveref} % cleverly cross referencing figures and tables; last package to include
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}

% To use for resetting the numbering of Appendix Tables and Figures:
%\setcounter{table}{0}
%\renewcommand{\thetable}{A\arabic{table}}
%\setcounter{figure}{0}
%\renewcommand{\thefigure}{A\arabic{figure}}

\makeatother
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={AJ Smit (University of the Western Cape)},
             pdfkeywords = {},
            pdftitle={What to do when assumptions about your data fail},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls


\begin{document}

% \maketitle

{% \usefont{T1}{pnc}{m}{n}
\setlength{\parindent}{0pt}
\thispagestyle{plain}
{\fontsize{18}{20}\selectfont\raggedright
\maketitle  % title \par
}
{
   \vskip 13.5pt\relax \normalsize\fontsize{11}{12}
\textbf{\authorfont AJ Smit} \hskip 15pt \emph{\small University of the
Western Cape}   
}
}



\vskip 6.5pt

\noindent 

\section*{Normal and non-normal data}

Throughout the preceding sections I have stressed the importance of
testing the assumptions underlying some statistical tests, in particular
\emph{t}-tests, ANOVAs, regressions, and correlations. These statistics
are called \emph{parametric statistics} and they require that the
assumption of normality and homogeneity of variances are met. This is
the kind of statistic you would normally be required to calculate, and
because they are commonly used, most people are familiar with parametric
statistics. However, when the data are not normal (i.e.~skewed) or the
variances are unequal --- as sometimes happens --- the resultant
parametric test statistics cannot be used. When this happens, we have
two options:

\begin{enumerate}
\item apply the \emph{non-parametric} equivalent for the statistical test in question, or
\item transform the data.
\end{enumerate}

It is the intention of this Chapter to discuss some options for data
transformations. But before we do that, please revise the non-parametric
options available as replacements for the main parametric approaches as
may be seen in our
\href{https://ajsmit.github.io/Basic_stats/index.html}{online textbook}
and the succinct summary presented in the Methods Cheatsheet.

\subsection*{Data transformations}

If the measurement variable is non-Gaussian (not normal) and/or has widely different
variances in different levels of a treatment, you could apply a data
transformation. To transform data, one performs a mathematical operation
on each observation in a measurement variable, then use these
transformed data in the statistical test. Afterwards, back transform the
summary statistics such as the mean or median (including the measures of
variance, such as SD and CI) to the original units before reporting it
in tables and figures or other means of summary.

Below (i.e.~the text on log transformation, square-root transformation,
and arcsine transformation) I have extracted mostly verbatim the
excellent text produced by John H MacDonald from his
\href{http://www.biostathandbook.com/transformation.html}{Handbook of
Biological Statistics}. Please attribute this text directly to him. I
have made minor editorial changes to point towards some R code, but
aside from that the text is more-or-less used verbatim. I strongly
suggest reading the preceding text under his
\href{http://www.biostathandbook.com/transformation.html}{Data
transformations} section, as well as consulting the textbook for
in-depth reading about biostatistics. Highly recommended!

\begin{description}
\item[Log transformation] This consists of taking the log of each observation. You can use either base-10 logs (\texttt{log10(x)}) or base-$e$ logs, also known as natural logs (\texttt{log(x)}). It makes no difference for a statistical test whether you use base-10 logs or natural logs, because they differ by a constant factor; the base-10 log of a number is just 2.303…× the natural log of the number. You should specify which log you're using when you write up the results, as it will affect things like the slope and intercept in a regression. I prefer base-10 logs, because it's possible to look at them and see the magnitude of the original number: $log(1) = 0$, $log(10) = 1$, $log(100) = 2$, etc.

The back transformation is to raise 10 or $e$ to the power of the number; if the mean of your base-10 log-transformed data is 1.43, the back transformed mean is $10^{1.43} = 26.9$ (in R, \texttt{10\^{}1.43}). If the mean of your base-$e$ log-transformed data is 3.65, the back transformed mean is $e^{3.65} = 38.5$ (in R, \texttt{exp(3.65)}). If you have zeros or negative numbers, you can't take the log; you should add a constant to each number to make them positive and non-zero (i.e. \texttt{log10(x) + 1}). If you have count data, and some of the counts are zero, the convention is to add 0.5 to each number.

Many variables in biology have log-normal distributions, meaning that after log-transformation, the values are normally distributed. This is because if you take a bunch of independent factors and multiply them together, the resulting product is log-normal. For example, let's say you've planted a bunch of maple seeds, then 10 years later you see how tall the trees are. The height of an individual tree would be affected by the nitrogen in the soil, the amount of water, amount of sunlight, amount of insect damage, etc. Having more nitrogen might make a tree 10\% larger than one with less nitrogen; the right amount of water might make it 30\% larger than one with too much or too little water; more sunlight might make it 20\% larger; less insect damage might make it 15\% larger, etc. Thus the final size of a tree would be a function of nitrogen × water × sunlight × insects, and mathematically, this kind of function turns out to be log-normal.

\item[Square-root transformation] This consists of taking the square root of each observation. The back transformation is to square the number. If you have negative numbers, you can't take the square root; you should add a constant to each number to make them all positive.

People often use the square-root transformation when the variable is a count of something, such as bacterial colonies per Petri dish, blood cells going through a capillary per minute, mutations per generation, etc.

\item[Arcsine transformation] This consists of taking the arcsine of the square root of a number (in R, \texttt{arcsin(sqrt(x))}). (The result is given in radians, not degrees, and can range from $\frac{-\pi}{2}$ to $\frac{\pi}{2}$.) The numbers to be arcsine transformed must be in the range 0 to 1. This is commonly used for proportions, which range from 0 to 1, [\textellipsis] the back-transformation is to square the sine of the number (in R, \texttt{sin(x)\^{}2}).
\end{description}

These are by no means the only types of transformations available. Let
us classify the above transformations, and a few others, into categories
of the types of corrective actions needed:

\begin{itemize}

\item{Slightly skewed data}
\begin{itemize}
\item \texttt{sqrt(x)} for positively skewed data
\item \texttt{sqrt(max(x+1) - x)} or \texttt{x\^{}2} for negatively skewed data
\end{itemize}

\item{Moderately skewed data}
\begin{itemize}
\item \texttt{log10(x)} for positively skewed data,
\item \texttt{log10(max(x + 1) - x)} or \texttt{x\^{}3} for negatively skewed data
\end{itemize}

\item{Severely skewed data}
\begin{itemize}
\item \texttt{1/x} for positively skewed data
\item \texttt{1/(max(x + 1) - x)} or higher powers than cubes for negatively skewed data
\end{itemize}

\item{Deviations from linearity and heteroscedasticity}
\begin{itemize}
\item \texttt{log(x)} when the dependent variable starts to increase more and more rapidly with increasing independent variable values
\item \texttt{x\^{}2} when the dependent variable values decrease more and more rapidly with increasing independent variable values
\item Regression models do not necessarily require data transformations to deal with heteroscedasticity. \emph{Generalised Linear Models} (GLM) can be used with a variety of variance and error structures in the residuals via so-called link functions. Please consult the \texttt{glm()} function for details.
\item The linearity requirement specifically applies to linear regressions. However, regressions do not \emph{have} to be linear. Some degree of curvature can be accommodated by additive (polynomial) models, which are like linear regressions, but with additional terms (you already have the knowledge you need to fit such models). More complex departures from linearity can be modelled by non-linear models (e.g. exponential, logistic, Michaelis-Menten, Gompertz, von Bertalanffy and their ilk) or \emph{Generalised Additive Models} (GAM) --- these more complex relationships will not be covered in this module. The \texttt{gam()} function in the \textbf{mgcv} package fits GAMs. After fitting these parametric or semi-parametric models to accommodate non-linear regressions, the residual error structure still does to meet the normality requirements, and these can be tested as before with simple linear regressions.
\end{itemize}

\end{itemize}

Knowing how to successfully implement transformations can be as much art
as science and requires a great deal of experience to get right. Due to
the multitude of options I cannot offer comprehensive examples to deal
with all eventualities --- so I will not provide any examples at all! I
suggest reading widely on the internet or textbooks, and practising by
yourselves on your own datasets.

\end{document}
