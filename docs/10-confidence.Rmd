---
title: "Confidence intervals"
author: "AJ Smit"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
  self_contained: false
  cards: false
  thumbnails: true
  lightbox: false
  gallery: true
  use_bookdown: true
  highlight: tango
---

<!-- # Confidence intervals --->

```{r prelim-opts, echo=FALSE}
knitr::opts_chunk$set(
  comment = "R>", 
  warning = FALSE, 
  message = FALSE,
  out.width = '70%'
)
```

A confidence interval (CI) tells us within what range we may be certain to find the true mean from which any sample has been taken. If we were to repeatedly sample the same population over and over and calculated a mean every time, the 95% CI indicates the range that 95% of those means would fall into.

## Calculating confidence

```{r}
Input <- ("
Student  Grade     Teacher  Steps  Rating
a        Gr_1     Putin     8000   7
b        Gr_1     Putin     9000  10
c        Gr_1     Putin    10000   9
d        Gr_1     Putin     7000   5
e        Gr_1     Putin     6000   4
f        Gr_1     Putin     8000   8
g        Gr_10    Putin     7000   6
h        Gr_10    Putin     5000   5
i        Gr_10    Putin     9000  10
j        Gr_10    Putin     7000   8
k        Gr_1     Sadam     8000   7
l        Gr_1     Sadam     9000   8
m        Gr_1     Sadam     9000   8
n        Gr_1     Sadam     8000   9
o        Gr_10    Sadam     6000   5
p        Gr_10    Sadam     8000   9
q        Gr_10    Sadam     7000   6
r        Gr_1     Donald   10000  10
s        Gr_1     Donald    9000  10
t        Gr_1     Donald    8000   8
u        Gr_1     Donald    8000   7
v        Gr_1     Donald    6000   7
w        Gr_10    Donald    6000   8
x        Gr_10    Donald    8000  10
y        Gr_10    Donald    7000   7
z        Gr_10    Donald    7000   7
")

data <- read.table(textConnection(Input), header = TRUE)
summary(data)
```

The package **rcompanion** has a convenient function for estimating the confidence intervals for our sample data. The function is called `groupwiseMean()` and it has a few options (methods) for estimating the confidence intervals, e.g. the 'traditional' way using the *t*-distribution, and a bootstrapping procedure.

Let us produce the confidence intervals using the traditional method for the group means:

```{r}
library(rcompanion)
# Ungrouped data are indicated with a 1 on the right side of the formula,
# or the group = NULL argument; so, this produces the overall mean
groupwiseMean(Steps ~ 1, data = data, conf = 0.95, digits = 3)

# One-way data
groupwiseMean(Steps ~ Grade, data = data, conf = 0.95, digits = 3)

# Two-way data
groupwiseMean(Steps ~ Teacher + Grade, data = data, conf = 0.95, digits = 3)
```

Now let us do it through bootstrapping:

```{r}
groupwiseMean(Steps ~ Grade,
              data = data,
              conf = 0.95,
              digits = 3,
              R = 10000,
              boot = TRUE,
              traditional = FALSE,
              normal = FALSE,
              basic = FALSE,
              percentile = FALSE,
              bca = TRUE)

groupwiseMean(Steps ~ Teacher + Grade,
              data = data,
              conf = 0.95,
              digits = 3,
              R = 10000,
              boot = TRUE,
              traditional = FALSE,
              normal = FALSE,
              basic = FALSE,
              percentile = FALSE,
              bca = TRUE)
```

These upper and lower limits may then be used easily within a figure.

```{r, fig.cap="A very basic figure showing confidence intervals (CI) for a random normal distribution.", message=FALSE, warning=FALSE}
# Load libraries
library(tidyverse)

# Create dummy data
r_dat <- data.frame(value = rnorm(n = 20, mean = 10, sd = 2),
                    sample = rep("A", 20))

# Create basic plot
ggplot(data = r_dat, aes(x = sample, y = value)) +
  geom_errorbar(aes(ymin = mean(value) - sd(value), ymax = mean(value) + sd(value))) +
  geom_jitter(colour = "firebrick1")
```

## CI of compared means

AS stated above, we may also use CI to investigate the difference in means between two or more sample sets of data. We have already seen this in the ANOVA Chapter, but we shall look at it again here with our now expanded understanding of the concept.

```{r, fig.cap="Results of a post-hoc Tukey test showing the confidence interval for the effect size between each group."}
# First calculate ANOVA of seapl length of different iris species
iris_aov <- aov(Sepal.Length ~ Species, data = iris)

# Then run a Tukey test
iris_Tukey <- TukeyHSD(iris_aov)

# Lastly use base R to quickly plot the results
plot(iris_Tukey)
```

> **Task 1:** Judging from the figure above, which species have significantly different sepal lengths?

## Harrell plots

The most complete use of CI that we have seen to date is the Harrell plot. This type of figure shows the distributions of each sample set in the data as boxplots in a lower panel. In the panel above those boxplots it then lays out the results of a post-hoc Tukey test. This very cleanly shows both the raw data as well as high level statistical results of the comparisons of those data. Thanks to the magic of the Internet we may create these figures with a single line of code. This does however require that we load several new libraries.

```{r, fig.cap="Harrell plot showing the distributions of stipe lengths (cm) of the kelp _Ecklonia maxima_ at two different sites in the bottom panel. The top panel shows the confidence interval of the effect of the difference between these two sample sets based on a post-hoc Tukey test."}
# The easy creation of these figures has quite a few dependencies
library(lsmeans)
library(Hmisc)
library(broom)
library(car)
library(data.table)
library(cowplot)
source("../data/fit_model.R")
source("../data/make_formula_str.R")
source("../data/HarrellPlot.R")

# Load data
ecklonia <- read.csv("../data/ecklonia.csv")

# Create Harrell Plot
HarrellPlot(x = "site", y = "stipe_length", data = ecklonia, short = T)[1]
```

> **Task 2:** There are a lot of settings for `HarrePlot()`, what do some of them do?

The above figure shows that the CI of the difference between stipe lengths (cm) at the two sites does not cross 0. This means that there is a significant difference between these two sample sets. But let's run a statistical test anyway to check the results.

```{r}
# assumptions
ecklonia %>% 
  group_by(site) %>% 
  summarise(stipe_length_var = var(stipe_length),
            stipe_length_Norm = as.numeric(shapiro.test(stipe_length)[2]))
# We fail both assumptions...

# non-parametric test
wilcox.test(stipe_length ~ site, data = ecklonia)
```

The results of our Wilcox rank sum test, unsurprisingly, support the output of `HarrelPlot()`.

## Exercises

> **Exercise 1:** Create a combined tidy dataframe (observe tidy principles) with the estimates for the 95% CI for the teacher data, above, estimated using both the traditional and bootstrapping methods. Create a plot comprising two panels (one for the traditional estimates, one for the bootstrapped estimates) of the mean, median, scatter of raw data points, and the upper and lower 95% CI.

> **Exercise 2** Load a new dataset and create a Harrell plot from it based on values of your choosing. What does the Harrell plot show?
